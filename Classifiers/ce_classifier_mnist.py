# -*- coding: utf-8 -*-
"""TF_Classifier_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1msLhnpQT5nbmcEqqsdeXfFnGxHOGfBi2
"""

import tensorflow as tf
from keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import time
import os
from sklearn.metrics import f1_score

# Initializations #
num_classes = 10
channels = 1
height = 28
width = 28
onehot = np.eye(num_classes)

def get_data():
    mnist = tf.keras.datasets.mnist
    (x_train, y_train),(x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    return x_train, y_train, x_test, y_test

def get_imbalanced_train_data(drop_list, drop_percentage_list, x_train, y_train):
    x_train = x_train[:, :, :, np.newaxis]
    
    per_class_count = list()
    for c in range(num_classes):
        per_class_count.append(np.sum(np.array(y_train == c)))
    print ('Per class count before dropping: ')
    print (per_class_count) 

    for i in drop_list:
        class_num = drop_list[i]
        drop_percentage = drop_percentage_list[i]
        all_ids = list(np.arange(len(x_train)))
        mask = [class_num == lc for lc in y_train]
        # mask is a boolean array of index where class_num is present in y_train
        all_ids_c = np.array(all_ids)[mask]
        np.random.shuffle(all_ids_c)
        # all_ids_c is shuffled np array of all indexes of class_num

        other_class_count = np.array(per_class_count)
        other_class_count = np.delete(other_class_count, class_num)
        to_keep = int(np.ceil((1. - drop_percentage) * per_class_count[class_num]))

        to_delete = all_ids_c[to_keep: len(all_ids_c)]

        x_train = np.delete(x_train, to_delete, axis=0)
        y_train = np.delete(y_train, to_delete, axis=0)

    unbalanced_per_class_count = list()
    for c in range(num_classes):
        unbalanced_per_class_count.append(np.sum(np.array(y_train == c)))
    
    print ('Per class count after dropping: ')
    print (unbalanced_per_class_count) 
    
    class_weights = []
    for nums in unbalanced_per_class_count:
        class_weights.append(np.sum(unbalanced_per_class_count)/nums)
        
    class_weights = np.asarray(class_weights, dtype = np.float32)
    inverse_class_weights = class_weights/np.sum(class_weights)
    inverse_class_weights = inverse_class_weights[:, np.newaxis]
    
    y_train = onehot[y_train.astype(np.int32)]
    
    return x_train, y_train, inverse_class_weights

def scale(x):
    x = (x-0.5)/0.5
    return x

def rescale(x):
    return np.asarray((x * 127.5 + 127.5).astype(np.uint8))

def tf_classifier(x, dropout_rate = 0.):
    
    with tf.variable_scope('Classifier', reuse = tf.AUTO_REUSE):
        print ("\n Classifier Architecture \n")
        
        print (x.shape)
        # ?*28*28*1
        
#         w_init = tf.keras.initializers.glorot_normal()
        # Layer 1
        conv1 = tf.layers.conv2d(x, 64, kernel_size = [4,4],
                                         strides = [1,1], padding = 'same', name = 'conv1')
        print (conv1.shape)
        # ?*28*28*64
        max_pool1 = tf.nn.max_pool(conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', name = 'pool1')
        dropout1 = tf.nn.dropout(max_pool1, dropout_rate, name = 'dropout1')
        act1 = tf.nn.relu(dropout1, name = 'act1')
        print (max_pool1.shape)
        # ?*14*14*64
        
        # Layer 2
        conv2 = tf.layers.conv2d(dropout1, 128, kernel_size = [4,4],
                                         strides = [1,1], padding = 'same', name = 'conv2')
        print (conv2.shape)
        # 14*14*128
        max_pool2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', name = 'pool2')
        dropout2 = tf.nn.dropout(max_pool2, dropout_rate, name = 'dropout2')
        act2 = tf.nn.relu(dropout2, name = 'act2')
        print (max_pool2.shape)
        # ?*7*7*128
        
        # Layer 3
#         flatten = tf.reduce_mean(act, axis = [1,2], name = 'flatten')
#         print (flatten.shape)
#         # ?*256
        
        flatten = tf.reshape(act2, [-1, 7*7*128], name = 'flatten')
        
        logits = tf.layers.dense(flatten, 10, name = 'logits')
        print (logits.shape)

        out = tf.nn.softmax(logits, name = 'out')
        
        return logits, out

# def tf_classifier(x, dropout_rate = 0.):
    
#     with tf.variable_scope('Classifier', reuse = tf.AUTO_REUSE):
#         print ("\n Classifier Architecture \n")
        
#         print (x.shape)
#         # 28*28*1
        
#         w_init = tf.keras.initializers.glorot_normal()
#         # Layer 1
#         conv1 = tf.layers.conv2d(x, 128, kernel_size = [4,4],
#                                          strides = [2,2], padding = 'same', name = 'conv1', kernel_initializer = w_init)
#         print (conv1.shape)
#         # 14*14*1
#         max_pool1 = tf.nn.max_pool(conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', name = 'pool1')
#         dropout1 = tf.nn.dropout(max_pool1, dropout_rate, name = 'dropout1')
#         act1 = tf.nn.leaky_relu(dropout1, name = 'act1')
#         print (act1.shape)
#         # ?*7*7*128
        
#         # Layer 2
#         conv2 = tf.layers.conv2d(act1, 256, kernel_size = [2,2],
#                                          strides = [2,2], padding = 'valid', name = 'conv2', kernel_initializer = w_init)
#         print (conv2.shape)
#         # 3*3*256
#         max_pool2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID', name = 'pool2')
#         dropout2 = tf.nn.dropout(max_pool2, dropout_rate, name = 'dropout2')
#         act2 = tf.nn.leaky_relu(dropout2, name = 'act2')
#         print (act2.shape)
#         # ?*1*1*256
        
#         # Layer 3
#         flatten = tf.reduce_mean(act2, axis = [1,2], name = 'flatten')
#         print (flatten.shape)
#         # ?*256
        
#         logits = tf.layers.dense(flatten, 10, name = 'logits')
#         print (logits.shape)
#         out = tf.nn.softmax(logits, name = 'out')
        
#         return logits, out

def loss_fnc(mask, x, y_label, dropout_rate):
    logits, out = tf_classifier(x, dropout_rate)
    with tf.variable_scope('Loss_fnc', reuse = tf.AUTO_REUSE):
      total_loss = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y_label)
      imb_loss = tf.multiply(mask, total_loss)
      imb_loss = tf.reduce_mean(imb_loss)
      class_loss = tf.reduce_mean(total_loss, name = 'class_loss') 
    return class_loss, out, imb_loss, class_loss-imb_loss

def optimizer(class_loss, lr, global_step, beta1 = 0.5):
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        all_vars = tf.trainable_variables()
        classifier_vars = [var for var in all_vars if var.name.startswith('Classifier')]
        with tf.variable_scope('Optimizer', reuse = tf.AUTO_REUSE):
            train_opt = tf.train.AdamOptimizer(lr, beta1, name = 'classifier_optimiser').minimize(class_loss,
                                                                global_step = global_step, var_list = classifier_vars)
    return train_opt

############ Plotting Results ############
def show_train_hist(hist, show = False, save = False):
    
    
    x = range(len(hist['train_loss']))

    y1 = hist['train_loss']
    y2 = hist['test_loss']

    plt.plot(x, y1, label='train_loss')
    plt.plot(x, y2, label='test_loss')

    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    plt.legend(loc=4)
    plt.grid(True)
    plt.tight_layout()

    if save:
        fpath = path + 'Train_hist.png'
        plt.savefig(fpath)
        print("Histogram saved")

    if show:
        plt.show()
    else:
        plt.close()

def train_classifier(batch_size, epochs, drop_list, drop_percent):
    
    train_hist = {}
    train_hist['train_loss'] = []
    train_hist['test_loss'] = []
    train_hist['Epoch_time'] = []
    train_hist['imb_loss'] = []
    train_hist['bal_loss'] = []
    train_hist['test_accuracy'] = []
    train_hist['F1'] = []
    
    tf.reset_default_graph()
    
    # Inputs
    x = tf.placeholder(tf.float32, shape = [None, height, width, channels], name = 'x')
    y_label = tf.placeholder(tf.float32, shape = [None, num_classes], name = 'y_label')
    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')
    drop_mask = tf.placeholder(tf.float32,  shape = [batch_size,], name = 'drop_mask')
    
    # Optimizer: set up a variable that's incremented once per batch and
    # controls the learning rate decay.
    global_step = tf.Variable(0, trainable=True, name = 'global_step')
    # Decay once per epoch, using an exponential schedule starting at 0.0002
    lr = tf.train.exponential_decay(0.0002, global_step, 400, 0.95, staircase = True, name = 'lr')
    
    # Creating imbalanced dataset
    x_train, y_train, x_test, y_test = get_data()

    x_train, y_train, inverse_class_weights = get_imbalanced_train_data(drop_list, drop_percent, x_train, y_train)
    x_test, y_test, inverse_class_weights_test = get_imbalanced_train_data([], [], x_test, y_test)
    
    # Building the graph
    class_loss, out, imb_loss, bal_loss = loss_fnc(drop_mask, x, y_label, dropout_rate)
    train_opt = optimizer(class_loss, lr, global_step, beta1 = 0.5)
    
#     saver = tf.train.Saver(max_to_keep = 2)
    
    print ("\n... Training begins ...\n")
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        no_of_batches = int((x_train.shape[0]/batch_size))
               
        for epoch in range(epochs):
            
            epoch_start_time = time.time()
            train_losses = []
            imb_losses = []
            bal_losses = []
            
            
            for it in range(no_of_batches):
                
                # Discriminator update
                batch_x = x_train[it*batch_size : (it+1)*batch_size]
                batch_y = y_train[it*batch_size : (it+1)*batch_size] # ?*10
                
                #preparing mask
                mask = np.zeros(batch_size)
                batch_y_temp = np.argmax(batch_y, 1)
                for i in range(len(batch_y_temp)):
                  if batch_y_temp[i] in drop_list:
                    mask[i] = 1
                    
                train_feed_dict = {x:scale(batch_x), y_label:batch_y, dropout_rate:0.5, drop_mask:mask}
                
                train_opt.run(feed_dict = train_feed_dict)
                train_loss = class_loss.eval(feed_dict = train_feed_dict)
                imb_train_loss = imb_loss.eval(feed_dict = train_feed_dict)
                bal_train_loss = bal_loss.eval(feed_dict = train_feed_dict)
        
                train_losses.append(train_loss)
                imb_losses.append(imb_train_loss)
                bal_losses.append(bal_train_loss)
                
            epoch_end_time = time.time()
            epoch_time = epoch_end_time - epoch_start_time
            test_loss = class_loss.eval(feed_dict = {x:scale(x_test), y_label:y_test, dropout_rate:0.5})
            prediction = out.eval(feed_dict = {x:scale(x_test), y_label:y_test, dropout_rate:0.5})
            
            # Evaluate model
            y__test = np.argmax(y_test, 1)
            y__pred = np.argmax(prediction, 1)
            
            correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_test, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
            print ('After epoch: '+ str(epoch+1) + ' Total Train loss: ' + str(np.mean(train_losses)) +
                   ' Total_imbalanced_loss: ' + "{0:.20f}".format(np.mean(imb_losses)) + ' Total_balanced_loss: ' + "{0:.20f}".format(np.mean(bal_losses)) +
                   ' Test loss: ' + str(test_loss) +' Time taken for epoch: ' + str(epoch_time))
    
            testacc = accuracy.eval(feed_dict = {x:scale(x_test), y_label:y_test, dropout_rate:0.5})
            
            f1 = f1_score(y__test, y__pred, average = 'weighted')
            print ('F1 score: ' + str(f1))
            print ('Test Accuracy: ' + str(testacc) + '\n')

            train_hist['train_loss'].append(np.mean(train_losses))
            train_hist['imb_loss'].append(np.mean(imb_losses))
            train_hist['bal_loss'].append(np.mean(bal_losses))
            train_hist['test_loss'].append(test_loss)
            train_hist['Epoch_time'].append(epoch_time)
            train_hist['test_accuracy'].append(testacc)
            train_hist['F1'].append(f1)
        
        
        print ("\n... Training finish ...\n")
        print (" Total time taken : " + str(np.sum(train_hist['Epoch_time'])))
        show_train_hist(train_hist, show = True, save = False)
    sess.close()
    return train_hist

results = []
batch_size = 128
drop_list = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6, 7, 8]]
drop_percent_list = [[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], [0.99, 0.99], [0.99, 0.99, 0.99, 0.99, 0.99], [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99], [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99]]
for i in range(len(drop_list)):
  train_hist = train_classifier(batch_size, 100, drop_list[i], drop_percent_list[i])
  results.append(train_hist)

for i in range(len(results)):
  train_hist = results[i] 
  a = (np.mean(train_hist['imb_loss']))
  b = (np.mean(train_hist['bal_loss']))
  print ('Avg imb_loss: ' + str(a))
  print ('Avg bal_loss: ' + str(b))
  # print ('Ratio of imb:bal = ' + str(a/b))
  print ('Highest Test acc: ' + str(np.max(train_hist['test_accuracy'])))
  print ('Highest F1 score: ' + str(np.max(train_hist['F1'])))
  print ('\n')

